{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\ramak\\\\OneDrive\\\\Desktop\\\\P2\\\\Image-Caption-Generator'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"../\")\n",
    "%pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen = True)\n",
    "class ModelTrainingConfig:\n",
    "    model_dir: Path\n",
    "    pkl_file: Path\n",
    "    file_path: Path\n",
    "    saved_model_dir: Path\n",
    "    epochs: int\n",
    "    batch_size: int\n",
    "    learning_rate: float\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen = True)\n",
    "class TrainingConfig:\n",
    "    train_dir: Path\n",
    "    pkl_file: Path\n",
    "    saved_trained_model_dir: Path\n",
    "    epochs: int\n",
    "    batch_size: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True \n",
      " True\n"
     ]
    }
   ],
   "source": [
    "from src.ImageCaptionGenerator.constants import *\n",
    "from src.ImageCaptionGenerator.utils.common import read_yaml, create_directory, save_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self, config_filepath = CONFIG_FILE_PATH, params_filepath = PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directory([self.config.main_dir])\n",
    "\n",
    "    def get_model_training_config(self) -> ModelTrainingConfig:\n",
    "        model_config = self.config.model_training\n",
    "        create_directory([model_config.model_dir])\n",
    "        # create_directory([config.train_dir])\n",
    "\n",
    "        model_training_config = ModelTrainingConfig(\n",
    "            model_dir = model_config.model_dir,\n",
    "            pkl_file = model_config.pkl_file,\n",
    "            file_path = model_config.file_path,\n",
    "            saved_model_dir = model_config.saved_model_dir,\n",
    "            epochs = self.params.EPOCHS,\n",
    "            batch_size = self.params.BATCH_SIZE,\n",
    "            learning_rate = self.params.LEARNING_RATE\n",
    "        )\n",
    "\n",
    "        return model_training_config\n",
    "    \n",
    "    def get_training_config(self) -> TrainingConfig:\n",
    "        train_config = self.config.training\n",
    "        create_directory([train_config.train_dir])\n",
    "        # create_directory([config.train_dir])\n",
    "\n",
    "        training_config = TrainingConfig(\n",
    "            train_dir = train_config.train_dir,\n",
    "            pkl_file = train_config.pkl_file,\n",
    "            saved_trained_model_dir = train_config.saved_trained_model_dir,\n",
    "            epochs = self.params.EPOCHS,\n",
    "            batch_size = self.params.BATCH_SIZE,\n",
    "        )\n",
    "\n",
    "        return training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-14 18:24:59,128: INFO: utils: NumExpr defaulting to 8 threads.]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from src.ImageCaptionGenerator import logger\n",
    "import urllib.request as request\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "from glob import glob\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dropout, Embedding, Masking\n",
    "from tensorflow.keras.layers import Dense, Reshape\n",
    "from tensorflow.keras.layers import LSTM, Concatenate,BatchNormalization, Bidirectional, RepeatVector\n",
    "from tensorflow.keras.layers import Add\n",
    "from tensorflow.keras.applications import ResNet50, InceptionV3, VGG16\n",
    "import pickle\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from itertools import islice\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "import math\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import json\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_training:\n",
    "    def __init__(self, model_config: ModelTrainingConfig):\n",
    "        self.model_config = model_config\n",
    "\n",
    "\n",
    "\n",
    "    def preprocessing_captions(self):\n",
    "\n",
    "        caption_dict = {}\n",
    "        caption_list = []\n",
    "        c = 0\n",
    "        path = os.path.join(\"artifact\", 'captions_data.pkl')\n",
    "\n",
    "        with open(self.model_config.file_path, 'r') as f:\n",
    "            next(f)\n",
    "            captions = f.read()\n",
    "\n",
    "        \n",
    "        for line in tqdm(captions.split('\\n')):\n",
    "\n",
    "            token = line.split(',')\n",
    "\n",
    "            if len(line) < 2: continue\n",
    "\n",
    "            image_id, caption = token[0], token[1:]\n",
    "            # image_id = image_id.split('.')[0]\n",
    "            caption = \" \".join(caption)\n",
    "\n",
    "            if image_id not in caption_dict:\n",
    "                caption_dict[image_id] = []\n",
    "            caption_dict[image_id].append(caption)\n",
    "\n",
    "\n",
    "        logger.info(f\"Length of caption_dict: {len(caption_dict)}\")\n",
    "\n",
    "        keys_list = list(caption_dict.keys())[:2]\n",
    "        for key in keys_list:\n",
    "            print(key, caption_dict[key])\n",
    "\n",
    "\n",
    "        for k,v in caption_dict.items():\n",
    "            for i in range(len(v)):\n",
    "                sentence = v[i]\n",
    "                sentence = sentence.lower()\n",
    "                sentence = sentence.replace('[^A-Za-z]', '')\n",
    "                sentence = sentence.replace('\\s+', ' ')\n",
    "                words = sentence.split()\n",
    "                words = [word for word in words if len(word) > 1]\n",
    "                sentence = \"[start] \" + \" \".join(words) + \" [end]\"\n",
    "                v[i] = sentence\n",
    "\n",
    "\n",
    "        # keys_list = list(caption_dict.keys())[:2]\n",
    "        # for key in keys_list:\n",
    "        #     print(key, caption_dict[key])\n",
    "\n",
    "\n",
    "        for k in caption_dict:\n",
    "            for caption in caption_dict[k]:\n",
    "                caption_list.append(caption)\n",
    "\n",
    "        # print(sentence_list[:10])\n",
    "                \n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(caption_list)\n",
    "\n",
    "        max_length = max(len(caption.split()) for caption in caption_list)\n",
    "        vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "        logger.info(f\"Vocabulary Size: {vocab_size}, Max Length: {max_length}\")\n",
    "\n",
    "        # Saving Pickle file\n",
    "        logger.info(\"Saving Pickle file.................\")\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'caption_dict': caption_dict,\n",
    "                'max_length': max_length,\n",
    "                'vocab_size': vocab_size,\n",
    "                'tokenizer' : tokenizer\n",
    "            }, f)\n",
    "                \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def model_preparation(self):\n",
    "        logger.info(\"Model Preparation Started.............................\")\n",
    "\n",
    "        # Load preprocessed data\n",
    "        path = os.path.join(\"artifact\", 'captions_data.pkl')\n",
    "        with open(path, 'rb') as pkl:\n",
    "            data = pickle.load(pkl)\n",
    "\n",
    "        max_length = data['max_length']\n",
    "        vocab_size = data['vocab_size']\n",
    "        print(vocab_size, \"\\n\", max_length)\n",
    "\n",
    "\n",
    "        inputs1 = Input(shape=(2048,))\n",
    "        fe1 = Dropout(0.5)(inputs1)\n",
    "        fe2 = Dense(256, activation='relu')(fe1)\n",
    "        # LSTM sequence model\n",
    "        inputs2 = Input(shape=(max_length,))\n",
    "        se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "        se2 = Dropout(0.5)(se1)\n",
    "        se3 = LSTM(256)(se2)\n",
    "        # Merging both models\n",
    "        decoder1 = Add()([fe2, se3])\n",
    "        decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "        outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "        \n",
    "        optimizer = Adam(learning_rate=0.001)  # Set your desired learning rate\n",
    "\n",
    "        model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "        plot_model(model, to_file='model_plot.png', show_shapes=True)\n",
    "        img = plt.imread('model_plot.png')\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "\n",
    "        logger.info(\"Model saving................................\")\n",
    "        self.save_model(path=self.model_config.saved_model_dir, model=model)\n",
    "\n",
    "\n",
    "\n",
    "    def save_model(self, path: Path, model=tf.keras.Model):\n",
    "        model.save(path)\n",
    "        logger.info(\"Model Saved.............................\")\n",
    "        \n",
    "\n",
    "            \n",
    "\n",
    "    #features = image_features mapping = caption_dict\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training:\n",
    "    def __init__(self, train_config: TrainingConfig, model_config: ModelTrainingConfig):\n",
    "        self.train_config = train_config\n",
    "        self.model_config = model_config\n",
    "\n",
    "\n",
    "    def load_model(self):\n",
    "        self.model = tf.keras.models.load_model(self.model_config.saved_model_dir)\n",
    "\n",
    "    def training_model(self):\n",
    "        logger.info(\"Training Started.............................\")\n",
    "\n",
    "        # Load preprocessed data\n",
    "        path = os.path.join(\"artifact\", 'captions_data.pkl')\n",
    "        with open(path, 'rb') as pkl:\n",
    "            data = pickle.load(pkl)\n",
    "\n",
    "        path1 = os.path.join('features.p')\n",
    "        with open(path1, 'rb') as pkl:\n",
    "            image_features = pickle.load(pkl)\n",
    "\n",
    "        caption_dict = data['caption_dict']\n",
    "        max_length = data['max_length'] \n",
    "        vocab_size = data['vocab_size']\n",
    "        tokenizer = data['tokenizer']\n",
    "\n",
    "        print(len(image_features),\"\\n\",len(caption_dict),\"\\n\",vocab_size)\n",
    "\n",
    "\n",
    "        image_names = list(caption_dict.keys())\n",
    "        split = int(len(image_names) * 0.80) \n",
    "        train_data = image_names[:split]\n",
    "        validation_data = image_names[split:]\n",
    "\n",
    "        print(f\"\\nlength of train_data {len(train_data)}, length of validation_data {len(validation_data)}\")\n",
    "\n",
    "        \n",
    "\n",
    "        steps_per_epoch = (len(train_data) // self.train_config.batch_size) + 1\n",
    "        validation_steps = len(validation_data) // self.train_config.batch_size + 1\n",
    "        # steps = math.ceil(len(train_dict) / self.train_config.batch_size)\n",
    "        logger.info(f\"Number of steps per epoch: {steps_per_epoch}\")\n",
    "\n",
    "        if not isinstance(tokenizer, tf.keras.preprocessing.text.Tokenizer):\n",
    "            raise TypeError(\"Loaded tokenizer is not an instance of Tokenizer class.\")\n",
    "\n",
    "        \n",
    "\n",
    "        for i in range(self.train_config.epochs):\n",
    "            print(i,\"/\",self.train_config.epochs,\":\\n\")\n",
    "            train_generator = self.data_generator(caption_dict, image_features, tokenizer, max_length, vocab_size)\n",
    "            self.model.fit_generator(\n",
    "            train_generator,\n",
    "            epochs=1,\n",
    "            steps_per_epoch=steps_per_epoch,\n",
    "            verbose=1\n",
    "            )\n",
    "            self.model.save(\"models/model_\" + str(i) + \".h5\")\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        self.save_model(path=self.train_config.saved_trained_model_dir, model=self.model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    def data_generator(self,train_data, image_features, tokenizer, max_length,vocab_size):\n",
    "        while 1:\n",
    "            for key, description_list in train_data.items():\n",
    "                #retrieve photo features\n",
    "                feature = image_features[key][0]\n",
    "                # print(description_list)\n",
    "                input_image, input_sequence, output_word = self.create_sequences(tokenizer, max_length, description_list, feature,vocab_size)\n",
    "                yield [[input_image, input_sequence], output_word]\n",
    "\n",
    "    def create_sequences(self, tokenizer, max_length, desc_list, feature, vocab_size):\n",
    "        X1, X2, y = list(), list(), list()\n",
    "        # walk through each description for the image\n",
    "        for desc in desc_list:\n",
    "            # encode the sequence\n",
    "            seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "            # split one sequence into multiple X,y pairs\n",
    "            for i in range(1, len(seq)):\n",
    "                # split into input and output pair\n",
    "                in_seq, out_seq = seq[:i], seq[i]\n",
    "                # pad input sequence\n",
    "                in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                # encode output sequence\n",
    "                out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                # store\n",
    "                X1.append(feature)\n",
    "\n",
    "                X2.append(in_seq)\n",
    "                y.append(out_seq)\n",
    "\n",
    "    \n",
    "\n",
    "        return np.array(X1), np.array(X2), np.array(y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def save_model(path: Path, model: tf.keras.Model):\n",
    "        model.save(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-14 18:41:38,370: INFO: common: yaml file: C:\\Users\\ramak\\OneDrive\\Desktop\\P2\\Image-Caption-Generator\\config\\config.yaml loaded]\n",
      "[2024-01-14 18:41:38,384: INFO: common: yaml file: C:\\Users\\ramak\\OneDrive\\Desktop\\P2\\Image-Caption-Generator\\params.yaml loaded]\n",
      "[2024-01-14 18:41:38,384: INFO: common: Created directory at: artifact]\n",
      "[2024-01-14 18:41:38,384: INFO: common: Created directory at: artifact/prepared_model]\n",
      "[2024-01-14 18:41:38,384: INFO: common: Created directory at: artifact/trained_model]\n",
      "[2024-01-14 18:41:40,761: INFO: 4152683301: Training Started.............................]\n",
      "8091 \n",
      " 8091 \n",
      " 8483\n",
      "\n",
      "length of train_data 6472, length of validation_data 1619\n",
      "[2024-01-14 18:41:41,015: INFO: 4152683301: Number of steps per epoch: 203]\n",
      "0 / 30 :\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ramak\\AppData\\Local\\Temp\\ipykernel_10212\\4152683301.py:52: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  self.model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203/203 [==============================] - 64s 302ms/step - loss: 6.3644 - accuracy: 0.0927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ramak\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 30 :\n",
      "\n",
      "203/203 [==============================] - 60s 294ms/step - loss: 5.1202 - accuracy: 0.1303\n",
      "2 / 30 :\n",
      "\n",
      "203/203 [==============================] - 59s 291ms/step - loss: 4.5896 - accuracy: 0.1698\n",
      "3 / 30 :\n",
      "\n",
      "203/203 [==============================] - 59s 291ms/step - loss: 4.1487 - accuracy: 0.1956\n",
      "4 / 30 :\n",
      "\n",
      "203/203 [==============================] - 60s 294ms/step - loss: 3.7756 - accuracy: 0.2173\n",
      "5 / 30 :\n",
      "\n",
      "203/203 [==============================] - 60s 294ms/step - loss: 3.4332 - accuracy: 0.2486\n",
      "6 / 30 :\n",
      "\n",
      "203/203 [==============================] - 72s 356ms/step - loss: 3.1342 - accuracy: 0.2807\n",
      "7 / 30 :\n",
      "\n",
      "203/203 [==============================] - 86s 422ms/step - loss: 2.8446 - accuracy: 0.3168\n",
      "8 / 30 :\n",
      "\n",
      "203/203 [==============================] - 86s 424ms/step - loss: 2.6226 - accuracy: 0.3409\n",
      "9 / 30 :\n",
      "\n",
      "203/203 [==============================] - 66s 323ms/step - loss: 2.3920 - accuracy: 0.3787\n",
      "10 / 30 :\n",
      "\n",
      "203/203 [==============================] - 60s 294ms/step - loss: 2.1739 - accuracy: 0.4190\n",
      "11 / 30 :\n",
      "\n",
      "203/203 [==============================] - 62s 304ms/step - loss: 2.0164 - accuracy: 0.4558\n",
      "12 / 30 :\n",
      "\n",
      "203/203 [==============================] - 60s 295ms/step - loss: 1.9010 - accuracy: 0.4692\n",
      "13 / 30 :\n",
      "\n",
      "203/203 [==============================] - 60s 296ms/step - loss: 1.8048 - accuracy: 0.4896\n",
      "14 / 30 :\n",
      "\n",
      "203/203 [==============================] - 60s 298ms/step - loss: 1.6791 - accuracy: 0.5182\n",
      "15 / 30 :\n",
      "\n",
      "203/203 [==============================] - 61s 299ms/step - loss: 1.5454 - accuracy: 0.5493\n",
      "16 / 30 :\n",
      "\n",
      "203/203 [==============================] - 61s 300ms/step - loss: 1.4438 - accuracy: 0.5709\n",
      "17 / 30 :\n",
      "\n",
      "203/203 [==============================] - 61s 298ms/step - loss: 1.3459 - accuracy: 0.5936\n",
      "18 / 30 :\n",
      "\n",
      "203/203 [==============================] - 61s 301ms/step - loss: 1.2332 - accuracy: 0.6203\n",
      "19 / 30 :\n",
      "\n",
      "203/203 [==============================] - 61s 299ms/step - loss: 1.1470 - accuracy: 0.6487\n",
      "20 / 30 :\n",
      "\n",
      "203/203 [==============================] - 61s 299ms/step - loss: 1.0527 - accuracy: 0.6674\n",
      "21 / 30 :\n",
      "\n",
      "203/203 [==============================] - 61s 300ms/step - loss: 0.9632 - accuracy: 0.6959\n",
      "22 / 30 :\n",
      "\n",
      "203/203 [==============================] - 62s 304ms/step - loss: 0.8795 - accuracy: 0.7218\n",
      "23 / 30 :\n",
      "\n",
      "203/203 [==============================] - 61s 299ms/step - loss: 0.7766 - accuracy: 0.7493\n",
      "24 / 30 :\n",
      "\n",
      "203/203 [==============================] - 61s 302ms/step - loss: 0.7022 - accuracy: 0.7691\n",
      "25 / 30 :\n",
      "\n",
      "203/203 [==============================] - 61s 302ms/step - loss: 0.6428 - accuracy: 0.7903\n",
      "26 / 30 :\n",
      "\n",
      "203/203 [==============================] - 61s 302ms/step - loss: 0.5916 - accuracy: 0.8071\n",
      "27 / 30 :\n",
      "\n",
      "203/203 [==============================] - 62s 305ms/step - loss: 0.5343 - accuracy: 0.8260\n",
      "28 / 30 :\n",
      "\n",
      "203/203 [==============================] - 61s 301ms/step - loss: 0.4876 - accuracy: 0.8448\n",
      "29 / 30 :\n",
      "\n",
      "203/203 [==============================] - 60s 297ms/step - loss: 0.4597 - accuracy: 0.8445\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_preparation_config = config.get_model_training_config()\n",
    "    training_config = config.get_training_config()\n",
    "    model_preparation = Model_training(model_config = model_preparation_config)\n",
    "    training_preparation = Training(train_config = training_config, model_config = model_preparation_config)\n",
    "    # model_preparation.preprocessing_captions()\n",
    "    # model_preparation.model_preparation()\n",
    "    # # print(len(train_dict))\n",
    "    training_preparation.load_model()\n",
    "    training_preparation.training_model()\n",
    "    # print(vocab_size,\"\\n\",max_length)\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
